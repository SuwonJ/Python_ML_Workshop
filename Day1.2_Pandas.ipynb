{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1.2 - Pandas and Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Load data files as `DataFrame` using Pandas.\n",
    "* Learn how to select and manipulate columns and rows in a Pandas `DataFrame`.\n",
    "* Apply core data wrangling techniques in `pandas`\n",
    "* Understand the flexibility of the `pandas` library\n",
    "* Be able to troubleshoot using documentations\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Introduction to Pandas\n",
    "\n",
    "* * * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id='df'></a>\n",
    "\n",
    "## Data Frames: Spreadsheets in Python\n",
    "\n",
    "Tabular data refers to data that is organized in a table with rows and columns.\n",
    "\n",
    "In scientific programming, tabular data is often called a **data frame**. In Python, the `pandas` package contains an object called `DataFrame` that implements this data structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importing Packages\n",
    "\n",
    "A **package** is a collection of pre-written code in a sharable format in Python, usually for specific functionalities. Packages are created to make coding easier by allowing us to reuse solutions that others have already developed. In this notebook, we will work with a package called `pandas`.\n",
    "\n",
    "Before using a package like `pandas`, we need to **import** it into our current Python session. Importing is done with the `import` keyword. By running `import [PACKAGE_NAME]`, we make the package’s functionality available for use in our code.\n",
    "\n",
    "For some packages, like `pandas`, we often use an **alias**, or a shorter nickname, when importing. This helps save time and makes our code cleaner when referring to the package.\n",
    "\n",
    "Now, let’s import the `pandas` package and assign it the alias `pd`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ImportError`\n",
    "\n",
    "If you encounter an `ImportError` when trying to import a package, it means that package has not been installed in your Python environment or is not accessible from your current environment. \n",
    "\n",
    "You can resolve this by using the command `!conda install package_name` directly within a Jupyter cell, replacing `package_name` with the name of the package you need. \n",
    "\n",
    "Why the exclamation mark `!`? It allows you to run **shell commands** from within a Jupyter Lab notebook. A **shell command** is a simple instruction you type to tell your computer to do something, like installing a program or managing files.In Jupyter Lab, you can use these instructions by starting with an exclamation mark `!` in a cell to make your computer do tasks, like installing a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install pandas\n",
    "#!conda install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦾 Challenge 1: From Dictionary to Data Frame\n",
    "\n",
    "You can easily build a data frame from a dictionary. However, the following code gives an error. Why does it have an error? \n",
    "\n",
    "💡 **Tip:** Google the line at the bottom of the error message if you need help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = ['Lee', 'Kim']\n",
    "sex = ['M', 'F', 'F']\n",
    "age = [40, 76, 46]\n",
    "\n",
    "medical_dict = {\n",
    "    'patient': patient,\n",
    "    'sex': sex,\n",
    "    'age': age}\n",
    "\n",
    "pd.DataFrame(medical_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Our Data\n",
    "\n",
    "Throughout the course, we are going to use a dataset [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction). This dataset was created by combining different datasets on heart diseases and patient infomraiton. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we use the `read_csv()` method, which takes a string as its main argument. This string consists of a **relative file path** pointing to the file. It is **relative** because we are referring to the location of the file in relation to the current working directory, rather than using an absolute path that specifies the entire directory structure from the root.\n",
    "\n",
    "💡 **Tip:** a directory is also called a folder.\n",
    "\n",
    "* `../` means \"go up one folder from where this notebook is\" -- which is the \"lessons\" folder.\n",
    "* `data/` means 'go into a folder called \"data\".\n",
    "* `heart.csv` is the file name we are accessing within that \"data\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip:** When dealing with file paths, especially in complex directory structures, remember that relative paths like `../folder_name` only take you up one directory level at a time. If you need to traverse multiple levels, you can chain these commands together (e.g., `../../folder_name` to go up two levels). \n",
    "\n",
    "Additionally, if your path requires navigating through several folders, consider using an **absolute path** from the home folder (`~/`), or a well-defined environment variable that points to a common base folder. This approach reduces errors and makes your code more portable across different systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.head()` method will show the first five rows of a Data Frame by default. \n",
    "\n",
    "💡 **Tip**: Put an integer in between the parentheses to specify a different number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on .csv Files\n",
    "As data scientists, we'll often be working with these **Comma Seperated Values (.csv)** files. \n",
    "\n",
    "Comma separated values files are commonly used because they are relatively small and easily readable in spreadsheet programs or text readers. A comma separated values file is just a text file that contains data but that has commas (or other separators) to indicate column breaks.\n",
    "\n",
    "`Pandas` package contains a function [`read_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).\n",
    "\n",
    "If your data is in a different format like `tsv` or `json`, we can either change the parameters of `read_csv()` method or use a differnet method like [`read_json()`](https://pandas.pydata.org//docs/reference/api/pandas.read_json.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='columns'></a>\n",
    "\n",
    "## Selecting Columns\n",
    "Now that we have our `DataFrame`, we can select a single column by selecting the name of that column. This uses bracket notation (like we do when accessing lists).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ChestPainType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type of this column is a `Series`. It's like a list. You can index a `Series` object just like you can with a list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pain_type = df['ChestPainType']\n",
    "pain_type[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Information\n",
    "\n",
    "* Age: age of the patient [years]\n",
    "* Sex: sex of the patient [M: Male, F: Female]\n",
    "* ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
    "* RestingBP: resting blood pressure [mm Hg]\n",
    "* Cholesterol: serum cholesterol [mm/dl]\n",
    "* FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
    "* RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
    "* MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n",
    "* ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n",
    "* Oldpeak: oldpeak = ST [Numeric value measured in depression]\n",
    "* ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
    "* HeartDisease: output class [1: heart disease, 0: Normal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Methods on Columns\n",
    "\n",
    "`DataFrame` objects come with their own methods, many of which operate on a single column of the DataFrame. \n",
    "\n",
    "For example, we can identify the number of unique values in each column by using the `nunique()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ChestPainType'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, a package provides **documentation** that explains all of its functionalities. Let's have a look at the documentation for a method called `value_counts()` [online](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html). \n",
    "\n",
    "🔔 **Question**: What does `value_counts()` do in the code below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['ChestPainType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 2: Putting Methods in Order\n",
    "\n",
    "In the following code we want to to find the top-3 most frequently occurring continents in our data. Put the following code fragments in the right order to get this information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".head(3)\n",
    ".value_counts()\n",
    "df['ChestPainType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes \n",
    "\n",
    "You can also look at certain properties of data, called **attributes**, using `pandas`. \n",
    "\n",
    "Attributes are like variables: they give you more information about the data that you have. Methods are like functions: they allow you to do something with data.\n",
    "\n",
    "For instance, we can easily check the column names of our data frame using the `columns` **attribute**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔔 **Question**: Here's another popular attribute: `shape`. What do you think it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Autocomplete\n",
    "\n",
    "Jupyter Notebooks allow for tab completion, just like many text editors. If you begin typing the name of something (such as a variable) that already exists, you can simply hit **Tab** and Jupyter will autocomplete it for you. If there is more than one possibility, it will show them to you and you can choose from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔔 **Question:** Below we are selecting a column in our `DataFrame`. See what happens when you hit `TAB`! What are you seeing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "df['ChestPainType']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rows'></a>\n",
    "\n",
    "## Selecting Rows\n",
    "\n",
    "What if we wanted to get some rows in our dataset based on some condition? For example, what if we just wanted a select only the rows for which the chest pain type is Atypical Angina? Or only rows from a particular age?\n",
    "\n",
    "We can use so-called **value comparison operators** for this. For instance, to get only the rows that include data points for `ATA`, we can use `==`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ChestPainType'] == 'ATA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip**: Fancy terminology alert: the above Series is called a **Boolean mask**. It's like a list of True/False labels that we can use to filter our Data Frame for a certain condition! We'll cover this further in Python Intermediate.\n",
    "\n",
    "Here, we create a subset of our Data Frame with the fancy Boolean mask we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting only the data points for ATA\n",
    "df[df['ChestPainType'] == 'ATA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of this operation is a **new data frame**! We can assign it to a new variable so we can work with this subsetted data frame. Let's do it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ata_df = df[df['ChestPainType'] == 'ATA']\n",
    "ata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 🦾 Challenge 3: Subsetting Data Frames\n",
    "\n",
    "Besides `==` we can use [other operators](https://www.w3schools.com/python/gloss_python_comparison_operators.asp) to compare values. For instance:\n",
    "- `<` less than\n",
    "- `>` greater than\n",
    "\n",
    "Fill in the code below to subset our data frame to include only people with `Age` less than 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "df[df[...] < ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦾 Challenge 4: Subsetting and Calculating the Mean\n",
    "\n",
    "Let's make use of subsetting to do some calculation! Calculate the **mean age** for a cheset pain type of your choice. \n",
    "\n",
    "This means you will have to:\n",
    "1. Subset the `ChestPainType` column using a Boolean mask.\n",
    "2. Take the `Age` column from that subset.\n",
    "3. Apply a Pandas method to get the mean from that column.\n",
    "\n",
    "You might not know how to get the mean of a column – yet! If that's the case, **use your search engine**.\n",
    "\n",
    "1. Enter the name of the computer language or package, and your question (for instance: \"Pandas calculate mean\").\n",
    "2. Read and compare the results you find.\n",
    "3. Try 'em out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions in Data Frames\n",
    "\n",
    "Remeber making your own functions? We can apply our custom functions to data frames as well. \n",
    "\n",
    "We can do this because Pandas makes use of so-called **vectorized** operations. This is just a fancy term for operations that can be performed on an entire row or column. \n",
    "\n",
    "For instance, if we'd want to add 10 to each value of `df['Age']`, we would simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what's happening to our DataFrame when we perform that operation:\n",
    "<img src=\"../images/vectorized.png\" alt=\"Vectorization in Pandas with scalar\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to `apply()` a Function\n",
    "\n",
    "We can also use a more general technique: `apply()`. `apply()` takes any function and **applies** it over every entry of the column. Let's see how we can do the same operation as above using `apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_10(x):\n",
    "    return x + 10\n",
    "\n",
    "df['Age'].apply(add_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦾 Challenge 5: `apply()` a Conditional Function\n",
    "\n",
    "Say that we want to create a new column in our dataset that classifies our datapoints in terms of the level of cholesterol level using the [CDC guidlines](https://www.cdc.gov/cholesterol/about/index.html). A total cholesterol level of less than 200 mg/dL (5.17 mmol/L) is normal. A total cholesterol level of 200 to 239 mg/dL (5.17 to 6.18 mmol/L) is borderline high. A total cholesterol level of 240 mg/dL (6.21 mmol/L) or greater is high. \n",
    "\n",
    "A good way to approach these kinds of problems is to write down all the steps you need to take. Then, you write your code by following the steps. In this case, we need to do the following:\n",
    "\n",
    "1. Start a function called `assign_level` that takes in one parameter, `i`.\n",
    "2. Write an if-elif-else statement that checks `i`, based on the following rules:\n",
    "    - `if` it is more than 240, `return` the string `very high`. \n",
    "    - `elif` it is more than 200, `return` the string `borderline high`. \n",
    "    - `elif` it is more than 150, `return` the string `normal`. \n",
    "    - `elif` it is less than or equal to 150, `return` the string `low`. \n",
    "    - `else`, return `np.nan` (this is a NaN value).\n",
    "3. Use `.apply()` on the `Cholesterol` column, using your new `assign_level` function as the argument. Assign the output to a new column in our DataFrame, called `Cholesterol_level`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done this correctly, the following code should produce a barplot of the different income levels in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cholesterol_level'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vector'></a>\n",
    "\n",
    "## Iteration: Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have data that really makes sense to apply vectorization for this data set. However, if you want to perform an operation between two columns, you don't have to create an iteration. In Pandas, we will can use [**vectorized**](https://www.geeksforgeeks.org/vectorized-operations-in-numpy) operations. We can just multiply two columns, and Pandas will know we want to multiply each row of both columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized = df['RestingBP'] * df['Cholesterol']\n",
    "vectorized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't make sense to mutliply these two columns and this is just to demostrate how this would work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Warning:** Note that the output to this operation is not a list, but a `Series` – a data type specific to Pandas. It is like a list, but it is **labeled**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized operations like these are really handy, and they replace most of the use of `for`-loops in a context of Pandas and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df_meth'></a>\n",
    "\n",
    "## Methods for `DataFrame` Objects\n",
    "\n",
    "Pandas has many methods: some allow you to work with entire DataFrames, while others operate on individual columns. This section focuses on learning to distinguish between these methods.\n",
    "\n",
    "Some methods work on entire DataFrames. We can look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) to see all the methods and attributes that are available for `DataFrame` objects. Learning how to read documentation is an important skill! \n",
    "\n",
    "## Summary Statistics\n",
    "The [`describe()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) method will give some summary statistics for a `DataFrame`. Run the cell below to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦾 Challenge 6: Check the Data Type\n",
    "\n",
    "What is the data type of the output of `describe()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plots\n",
    "\n",
    "Pandas has a convenient [`plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) method that allows you to create different kinds of visualizations. Some of these visualizations can be called on a DataFrame object.\n",
    "\n",
    "For instance, **scatter plots** visualize the relationship between different variables (columns) in a DataFrame. This is why we run the method on an entire DataFrame.\n",
    "\n",
    "We can create a scatter `plot()` by specifying the columns to use for the `x` and `y` axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='RestingBP', y='MaxHR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Values\n",
    "\n",
    "Let's say we want to find the heart with the highest `gdpPercap`.\n",
    "\n",
    "If we want to sort the values in a DataFrame we can use the [`sort_values()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) method on a DataFrame. It takes as an argument the column we want to sort the DataFrame on. \n",
    "\n",
    "⚠️ **Warning:** By default, `sort_values()` sorts in **ascending order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔔 **Question:** Add the argument `ascending=False` when running `sort_values()` in the cell above. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Plots\n",
    "\n",
    "Bar plots show the relationship between a numeric and a categoric variable. Here, we use the `ChestPainType` (categorical) and `RestingBP` (numeric) columns. Use a bar plot when you want to illustrate differences in frequencies of some category.\n",
    "\n",
    "In the below cell, we retrieve the 10 data points with the **low age** in our data using the `sort_values()` method, and then plot those data points in a bar plot.\n",
    "\n",
    "💡 **Tip**: Note that `plot.bar()` is a method of its own, and is an alternative to using `plot()` with the `type=bar` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort values based on low age, get top 10\n",
    "young = df.sort_values('Age', ascending=True)[:20]\n",
    "\n",
    "young_grouped = young.groupby('ChestPainType').max().reset_index()\n",
    "# Visualize with bar plot \n",
    "young_grouped.plot.bar(x='ChestPainType', y='RestingBP', figsize=(6,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='series_meth'></a>\n",
    "\n",
    "## Methods for `Series` Objects\n",
    "\n",
    "Some Pandas methods work on `Series` objects – single columns – instead of entire DataFrames.\n",
    "\n",
    "For instance, what if we wanted to calculate the median of life expectancy? We'd need to select just one column to operate on. \n",
    "\n",
    "Recall that we can select an individual column with bracket notation. This is analogous to indexing a list.\n",
    "\n",
    "🔔 **Question**: What is the type of the output of the below cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Cholesterol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single column of pandas is a `Series` object. This can be treated as a list or other iterable, and allows for you to do calculations over it. \n",
    "\n",
    "We can look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) to see the methods and attributes that are available for `Series` objects. If we want the median, we can use the `median()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Cholesterol'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Values\n",
    "\n",
    "There are many methods for summarizing DataFrames (which often are assigned as `df`). For instance, `value_counts()` returns a `Series` containing counts of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `value_counts()` also works on a single column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cholesterol'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "A histogram shows the distribution of a variable using binned values. We can call this using the syntax: `df[column].plot(kind='hist')`. Use a histogram if you want to show distributions of continuous variables.\n",
    "\n",
    "📝 **Poll PyFun 5-3:** Try changing the value for the `bins` parameter. What does the `bins` parameter seem to be determining?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Cholesterol'].plot(kind='hist', title='Histogram of Cholesterol', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🦾 Challenge 7: Combining loops with graphs\n",
    "1. Make a list of cheset pain types and store in a variable named `pain_type`.\n",
    "2. Complete to make function `plot_restingBP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Add comments\n",
    "pain_type = [..., ..., ...]\n",
    "\n",
    "# Add comments\n",
    "def plot_restingBP(..., ...):\n",
    "    for ... in ...:\n",
    "        heart_data = df[df['ChestPainType'] == ...]\n",
    "        plt.scatter(heart_data['Age'], heart_data['RestingBP'], label=...)\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Resting Blood Pressure (mm Hg)')\n",
    "    plt.title('Resting Blood Pressure by Age and Chest Pain Type')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Add comments\n",
    "plot_restingBP(df, pain_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip**: Notice we have added labels and title to the plot using  `plt.xlabel()`, `plt.ylabel()`, and `plt.title()`. See [this resource](https://www.w3schools.com/python/matplotlib_labels.asp) for more information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Python Data Wrangling with `pandas`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook, we provide an introduction to **data wrangling with Python**. We will be using the `pandas` package, which provides a rich set of tools for manipulating data. **Data wrangling** is the process of transforming and preparing raw data into a structured, clean, and usable format for analysis. It involves a series of tasks aimed at addressing inconsistencies, errors, or missing data while organizing the data to meet specific analytical requirements.\n",
    "\n",
    "We'll use worked examples and practice on real data to learn the core techniques of data wrangling -- how to index, manipulate, merge, group, and plot data -- in `pandas`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports pandas and assign it to the variable `pd`\n",
    "import pandas as pd\n",
    "\n",
    "# We often import NumPy (numerical python) with pandas\n",
    "# we will import that and assign it to the variable `np`\n",
    "import numpy as np\n",
    "\n",
    "# Load matplotlib for plotting later in the workshop\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` has a `read_csv()` function that allows us to easily import tabular data. The function returns a `DataFrame` object, which is the main object `pandas` uses to represent tabular data.\n",
    "\n",
    "Notice that we call `read_csv()` using the `pd` abbreviation from the import statement above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your code to import heart.csv data as heart here\n",
    "\n",
    "heart = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run `type()` on the `heart` object and see what it is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(heart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You've created a `pandas` `DataFrame`. We can look at our data by using the `.head()` method. By default, this shows the header (column names) and the first **five** rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 **Tip**: If you'd like to see some other number of rows, you can pass an integer to `.head()` to return that many rows. For example `heart.head(6)` would return the first six rows.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of rows, you can use the `.shape` attribute, which returns a [tuple](https://www.w3schools.com/python/python_tuples.asp): `(number of rows, number of columns)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out exactly what all of your columns are, you can use the `.columns` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out what kinds of data we have, we use the `.dtypes` attribute, which tells us which columns contain numerical data (e.g. `float64` or `int64` types) and which ones contain text (e.g. `object` types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking up functions\n",
    "A useful method that generates various summary statistics is `.describe()`. This is a powerful method that will return a lot of information, so before we run it, let's look up exactly what it does.\n",
    "\n",
    "💡 **Tip**: The [`pandas` documentation](http://pandas.pydata.org/pandas-docs/stable/) contains exhaustive information on every function, object, etc. in `pandas`. It can be a little difficult to navigate on its own, so it's typical to interact with the documentation primarily through Google searches.  \n",
    "\n",
    "The following is a general worflow for learning about a function in `pandas`:\n",
    "1. Google the `pandas` function, e.g. \"pandas {insert function name}\"\n",
    "2. Find a result from pandas.pydata.org (the pandas documentation)\n",
    "3. Read the summary of what the function does (at the top of the page), examine its arguments and what it returns.\n",
    "\n",
    "<span color=\"purple\">🔔 **Question:** Before running the following code, try using the general workflow detailed above to find out what `.describe()` returns. </span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Warning**: `.describe()` will behave differently depending on your data's types, or, `dtype`s. If your `DataFrame` includes both numeric and object (e.g., strings) `dtype`s, it will default to **summarizing only the numeric data** (as shown above). If `.describe()` is called on a `DataFrame` that only contains strings, it will return the count, number of unique values, and the most frequent value along with its count.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 1\n",
    "\n",
    "We have a similar heart data from Kaggle, named `heart_disease_uci.csv`.\n",
    "\n",
    "| **Feature**       | **Description**                                                                                  |\n",
    "|------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **id**          | Unique ID for each patient                                                                     |\n",
    "| **age**         | Age of the patient in years                                                                     |\n",
    "| **origin**      | Place of study                                                                                  |\n",
    "| **sex**         | Male/Female                                                                                     |\n",
    "| **cp**          | Chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])                   |\n",
    "| **trestbps**    | Resting blood pressure (in mm Hg on admission to the hospital)                                  |\n",
    "| **chol**        | Serum cholesterol in mg/dl                                                                      |\n",
    "| **fbs**         | Fasting blood sugar > 120 mg/dl (True/False)                                                    |\n",
    "| **restecg**     | Resting electrocardiographic results ([normal, stt abnormality, lv hypertrophy])                |\n",
    "| **thalach**     | Maximum heart rate achieved                                                                     |\n",
    "| **exang**       | Exercise-induced angina (True/False)                                                            |\n",
    "| **oldpeak**     | ST depression induced by exercise relative to rest                                              |\n",
    "| **slope**       | The slope of the peak exercise ST segment                                                       |\n",
    "| **ca**          | Number of major vessels (0-3) colored by fluoroscopy                                            |\n",
    "| **thal**        | [Normal; fixed defect; reversible defect]                                                       |\n",
    "| **num**         | The predicted attribute                                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Challenge**  \n",
    "Whenever we open a new DataFrame, it's important to get a basic understanding of its structure.\n",
    "\n",
    "Using the methods and attributes we just discussed, **answer the following questions** about `heart`:\n",
    "\n",
    "1. What columns does `heart_disease_uci.csv` contain?\n",
    "2. How many rows and columns does it contain?\n",
    "3. What are the minimum and maximum values of the columns with numerical data?\n",
    "\n",
    "<details><summary><a>Click for hint</a></summary>\n",
    "Hint: consider using <code>.columns</code>, <code>.shape</code>, and <code>.describe()</code> here.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='indexing'></a>\n",
    "# Indexing Data\n",
    "Wrangling data in a DataFrame often requires extracting specific rows and/or columns of interest. This is referred to as **Indexing**. We've actually already learned a simple way to index our data using `.head()`, which isolated the first five rows of our data. Now, we'll learn more flexible and powerful methods for indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall basic Python indexing\n",
    "To index (this is synonymous with other verbs like \"subset,\" \"slice,\" etc.) data in Python, we use bracket notation: `[]`. Run the following code to instantiate a list of numbers and observe what different indexes return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_list = ['a', 'b', 'c', 'd', 'e', 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_list[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Indexing works very similarly in `pandas` as it does in standard python, but with a few key differences. In `pandas`, indexing relies on referencing a DataFrame's rows and then its columns <span>&#8594;</span> `[rows, columns]`. Let's get a more visual sense of this -- in the `heart` DataFrame that we created earlier, the structure of the data is as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index and get to specific data from this DataFrame, we select a row/column combination.  \n",
    "For example, indexing row 3 and the column `ChestPainType` would give us the value 'ASY'. In code, that would look as follows:  \n",
    "`heart.loc[3, 'ChestPainType']`  \n",
    "Try writing that in the cell below and running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "heart.loc[3, 'ChestPainType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.loc`\n",
    "Let's go deeper into what `.loc` does, as this will be the primary tool we use for indexing.  \n",
    "\n",
    "`.loc` allows us to index data based on the labels of our DataFrame's index and its column names. Let's take a look at its behavior below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.loc[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code, `heart.loc[:4, :]` executes the following:  \n",
    "- <code>heart.loc[<mark style=\"background: yellow\">**:4**</mark>, :]</code> <span>&#8594;</span> Select rows up to index 4\n",
    "- <code>heart.loc[:4,<mark style=\"background: yellow\"> **:**</mark>]</code> <span>&#8594;</span>\n",
    " Select all columns\n",
    "\n",
    "This format allows us to flexibly select ranges of rows and columns at the same time. Consider this more complex example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.loc[2:4, 'ChestPainType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code executed the following:  \n",
    "- <code>heart.loc[<mark style=\"background: yellow\">**2:4**</mark>, 'ChestPainType']</code> -> Select rows from index 2 up to index 4 \n",
    "- <code>heart.loc[2:4, <mark style=\"background: yellow\">**'ChestPainType'**</mark>]</code> -> Select the `ChestPainType` column\n",
    "\n",
    "💡 **Tip**: Note that the output of this code looks different from our previous output! Because we selected a single column, our code returned a `Series` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(heart.loc[2:4, 'ChestPainType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(heart[['ChestPainType']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(heart['ChestPainType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one more example of `.loc`.  \n",
    "🔔 **Question:** Before running the following code block, can you anticipate what it will output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.loc[19:29, ['ChestPainType', 'Age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 2: Indexing with `.loc`\n",
    "\n",
    "Let's get a little practice with the `.loc` operator.  \n",
    "\n",
    "<span style=\"color:purple\">Select rows 10 through 20, then compute their average `Age` </span>\n",
    "<details>\n",
    "    <summary><a>Click for Hint</a></summary>\n",
    "    This can be done using <code>`.loc`</code> and <code>`.mean()`</code>, all in one line of code: <code>heart.loc[{your row selection}, {your column selection}].mean()</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "heart.loc[10:20, 'RestingBP'].mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Indexing\n",
    "`.loc` is a very powerful indexing system that can handle almost any indexing task you can imagine. However, as is typical in `pandas`, there is more than one way to get what you're looking for. \n",
    "\n",
    "When we are executing very simple indexing tasks, such as selecting a column, it is common to use the more succinct **positional indexing** system. Positional indexing allows us to omit `.loc`, but only allows us to select a row **OR** column index, whereas most of the indexing we just did using `.loc` involved both row **AND** column indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will work\n",
    "heart.loc[1:5, 'RestingBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the following code -- it will throw an error. You can \"comment out\" (put a # before the code) the first statement and \"un-comment\" (remove the # before the code) the second statement to see how `.loc` fixes the error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this won't work\n",
    "#heart[0, 'RestingBP']\n",
    "\n",
    "# this will work\n",
    "heart.loc[0, 'RestingBP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `iloc`\n",
    "\n",
    "Another widely used alternative to `.loc` is `.iloc`. **We recommend sticking to `.loc` while learning `pandas`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Masks\n",
    "\n",
    "A **boolean mask** is a fancy term, but it simply means using a list of boolean values (True or False) to filter or modify data in another list, or list-like structure.\n",
    "\n",
    "The term \"mask\" comes from the idea that the boolean values \"cover\" the original data, revealing only the elements corresponding to the True values in the mask.\n",
    "\n",
    "A boolean mask can be applied to lists, but also to more complex data structures like DataFrames. This allows us to easily filter or select data based on specific conditions.\n",
    "\n",
    "A Boolean mask returns a `Series` object containing `True` and `False` values you can then use for other purposes. \n",
    "\n",
    "Let's use a boolean mask now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boolean'></a>\n",
    "# Boolean Indexing\n",
    "Now that we've covered the basics of indexing, let's get into an extremely powerful extension -- \"**boolean indexing**.\" Boolean indexing refers to filtering data based on some logical test. The `pandas` implementation of boolean indexing can be a little jarring at first, so let's build up to it from scratch. First, recall how booleans and logical tests work in standard python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Machine\" == \"Machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Machine\" == \"Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7 > 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use that same style of logical test in `pandas` to execute boolean indexing.   \n",
    "\n",
    "## Example: find patients with symptoms\n",
    "Notice in the `heart` dataframe we have a column named `ChestPainType` that tells us whether the patient experienced chest pain or not. We're going to do a boolean indexing example on these first five rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller test dataframe\n",
    "# to show how boolean indexing works\n",
    "test = heart.loc[3:13, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use that column to filter our data down to only heart outside of the European Union. The steps are as follows:\n",
    "1. Select the column we will use as a filter: `test['ChestPainType']` or `test.loc[:, 'ChestPainType']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ChestPainType']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Determine which rows in that column are equal to \"ASY\" -- which denotes that the patient is did not experience chest pain: `test['ChestPainType'] == 'ASY'`. The output of this code is what's called a **boolean mask**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['ChestPainType'] == 'ASY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the boolean mask to index only those rows that satisfied the test: `test[test['ChestPainType'] == 'ASY']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test[test['ChestPainType'] == 'ASY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (test['ChestPainType'] == 'ATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's boolean indexing! We used a test for equality (`heart['ChestPainType'] == 'ASY'`), but we can use a variety of different tests and conditions to index our data.\n",
    "\n",
    "For example, we might want to find the patients with age greater than some threshold, such as 50 (note that we will go back to using the full `heart` DataFrame now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart['Age'] > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = heart[heart['Age'] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 3: Boolean Indexing\n",
    "\n",
    "Let's push our boolean indexing skills a little further with a challenge problem.\n",
    "1. Find the average age of heart in our data, assign it to the variable `average_high`\n",
    "2. Find heart that have \"above average\" age\n",
    "<details>\n",
    "    <summary><a>Click for Hint</a></summary>\n",
    "    Compute the average Age of the data: <code>heart['Age'].mean()</code> and save that to a variable <code>average_high</code>. Then, you can use that variable to create a boolean mask for indexing: <code>heart['Age'] > average_high</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "average_high = heart[\"Age\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = heart[\"Age\"] > average_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "long_heart1 = heart[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_heart2 = heart[heart[\"Age\"] > average_high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_heart1 == long_heart2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "# 🎬 Demo: Boolean Indexing with multiple conditions\n",
    "We won't have a challenge on this topic, but it's useful to know that we can boolean index using as many logical tests as we want by wrapping each test in parenthesis (`()`)and by using the AND operator (`&`) or the OR operator (`|`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the heart with Age greater than 25 but less than 30\n",
    "heart[(heart['Age'] >= 25) & (heart['Age'] <= 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the heart with Age greater than 30 or less than 0\n",
    "heart[(heart['Age'] > 30) | (heart['Age'] < 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: More advanced Pandas functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sections\n",
    "4. [Missing Data](#missing)\n",
    "5. [Sorting Values](#sorting)\n",
    "6. [Merging](#merging)\n",
    "7. [Grouping](#grouping)\n",
    "8. [Visualization](#viz)\n",
    "\n",
    "Let's start back up by importing our libraries and loading up our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the heart dataset \n",
    "heart = pd.read_csv('../data/heart_disease_uci.csv')\n",
    "\n",
    "#This is a similar dataset from heart.csv but slightly different. Source: https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data?select=heart_disease_uci.csv\n",
    "\n",
    "# This is some formatting that's out of scope\n",
    "#heart['date'] = pd.to_datetime(heart['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.sort_values(by = 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='missing'></a>\n",
    "# Missing Values\n",
    "When working with a new data source, it's good to get an idea of how much information is missing. `pandas` provides various methods for exploring and dealing with \"missing-ness\", one of which is `.isna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.isna()` method returns a corresponding boolean value for each entry in the `heart` DataFrame. In Python, `True` is equivalent to 1 and `False` is equivalent to 0. Thus, when we add up the results by column (with `.sum()`), we get a count for the **total** number of missing values by column::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a wide variety of approaches to dealing with missing data. One basic approach would be to drop any row with a missing heart rate record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.dropna(subset=['chol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply running the `.dropna()` method actually doesn't alter our data -- it makes a copy then drops the missing rows for that copy. In order to save our alteration, we'll need to re-define the `heart` DataFrame with our altered copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = heart.dropna(subset=['chol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**💡 Tip:** Note that the brackets `[]` are needed in the `dropna()` method because the subset parameter expects a list of column names. Even if you're specifying just one column, it still needs to be enclosed in brackets to indicate that it's a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sorting'></a>\n",
    "# Sorting Values\n",
    "\n",
    "We've been working with data about heart rates, so it would probably be useful to know what the highest heart rates are in this data. For this, we'll use the `sort_values()` method to sort the data. We chain `.head()` onto the end of this so that we only see the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.sort_values(by='age').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.sort_values(by='age', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a copy of the `DataFrame`, sorted in **descending** order (note `ascending=False`), and prints the first five rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='merging'></a>\n",
    "# Merging DataFrames\n",
    "\n",
    "You might run into a scenario where you have useful information split between two dataframes. For this section, we will pretend that we were given two data frames `patient_info` containing patient information and `test_result` containing heart-related tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info = heart[['id', 'age', 'sex']]\n",
    "test_result = heart[['id', 'cp', 'trestbps', 'chol', 'fbs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_info.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data we need is stored in two separate files, we'll want to merge the data somehow. Let's determine which column we can use to join this data by taking a look at `heart`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two DataFrames seem to have the same `id` column, with the integer codes for each patient. Let's try doing a merge of these two datasets based on the `id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patient_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` includes an easy-to-use merge function that accepts two DataFrames and a column to merge them on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged = pd.merge(patient_info, test_result, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heart_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.sort_values(by='age', ascending=False).head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grouping'></a>\n",
    "# Grouping and Aggregating Data\n",
    "\n",
    "What if we'd like to know how many observations exist for each country? To do so, we need to group the heart, then count how many times each one occurs. In other words, we're going to **group** our data **by** a specific column, and calculate some quantity within each group. The \"group-by\" operation is a fundamental technique used with tabular data.  \n",
    "\n",
    "\n",
    "(For those who have used spreadsheet software like Excel, you might recognize that we are essentially talking about making a \"Pivot-Table\")\n",
    "\n",
    "## Simple Grouping with `.value_counts()`\n",
    "For simple grouping operations, we can use the handy `.value_counts()` method. We typically run this on a single column, and it will return a table showing how many observations there are for each unique value in the column. The following graphic represents the basics of the operations.\n",
    "\n",
    "<img src=\"../images/valcounts.svg\" align=\"left\" width=\"400\" alt=\"diagram of pandas datafram\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged['cp'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex grouping with `.groupby()`\n",
    "What if we want to do something more complex, like find out **what was the average heart rate for asymptomatic and non-asymptomatic?**. `.value_counts()` groups data then counts it, but we need a method that can group data then average it.\n",
    "\n",
    "This sort of question is a typical use case for `.groupby()` -- which allows us to group data then apply any **aggregate** function we want -- count, average, min, max, median, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we want to find out the average heart rate for asymptomatic and symptomatic patients, so we will group our data based on `cp`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the method, `.groupby()`. This doesn't actually return data or output -- it just groups the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heart_merged.groupby('cp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.groupby('cp').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to select a column of data and specify an aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.groupby('cp')['chol'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dissecting the code, we told `pandas`:\n",
    "1. <code>heart_merged<code><mark style=\"background: yellow\">.groupby('cp')</mark>['chol'].mean()</code>\n",
    "\n",
    "    Group all of our rows based on the unique values of the `cp` column\n",
    "    \n",
    "2. <code>heart_merged<code>.groupby('cp')<mark style=\"background: yellow\">['chol']</mark>.mean()</code>\n",
    "\n",
    "    Select the `chol` column\n",
    "\n",
    "2. <code>heart_merged<code>.groupby('cp')['chol']<mark style=\"background: yellow\">.mean()</mark></code>\n",
    "\n",
    "   Compute the average of the selected column (`chol`) for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.columns\n",
    "heart.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "heart.groupby('cp')[['trestbps','chol','thalch']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.groupby('cp').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_index = heart_merged['cp'] == 'asymptomatic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.loc[boolean_index, 'chol'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strengths of `.groupby()` relative to using boolean indexing are that `groupby()` scales very well to scenarios with many groups, and it requires much less code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 7\n",
    "\n",
    "Use `.groupby()` to find the maximum heart rate for each `cp` cateogry. Sort your results from largest to smallest.\n",
    "\n",
    "Use the example above for guidance. \n",
    "<details><summary><a>Click for hint</a></summary>\n",
    "1. First, use <code>groupby()</code> to group on \"cp\". <br>\n",
    "2. Then, select the \"chol\" column,<br>\n",
    "3. Aggregate by using <code>.max()</code> to get the max value.<br>\n",
    "4. Chain on the method <code>.sort_values(ascending=False)</code>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.groupby('cp')['chol'].min().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz'></a>\n",
    "# Data Visualization\n",
    "In the last challenge, you created an interesting table showing the all-time maximum heart rate that each country has experienced. Let's visualize that table as a bar chart to make it easier to present.\n",
    "\n",
    "There are various ways to approach data visualization in Python -- we'll cover simple plotting in `pandas`, which draws on functionality from the `matplotlib` library.\n",
    "\n",
    "First, we'll define a variable, `grouped`, with the table you just made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = heart_merged.groupby('cp')['chol'].max().sort_values(ascending=False)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot it. In `pandas`, visualization is as simple as calling the `.plot()` method, then supplying optional arguments (here I supplied `kind=\"barh\"` to make a horizontal bar chart rather than the default line-chart). The following is the maximum heart rate across heart in the data for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.plot(kind='barh')\n",
    "\n",
    "# We add plt.show() to properly render the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another plot. Our full DataFrame is what's called a time-series -- we have repeated observations of various heart' heart rates over time. We typically plot time-series using line-plots, so let's make a line plot examining Spain and Portugal's heart rates.  \n",
    "\n",
    "To make this sort of plot simpler, we'll start by making our date column into the DataFrame's index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged = heart_merged.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use boolean indexing to select on those observations that are for Males. We'll save that as a new DataFrame, `male`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = heart[heart['sex'] == 'Male']\n",
    "male.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily access Spain's heart rate, with the date for each observation included as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male['chol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same boolean indexing we used for Spain, but now for Portugal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = heart['sex'] == 'Female'\n",
    "female = heart[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "female.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take advantage of the `.plot()` function, simply calling `spain['heart_rate'].plot()`. We don't need to supply any argument to `.plot()` since we are using the default plot style -- a line-plot. We do add some other commands to add a y-axis label and render the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male['chol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male['chol'].plot(kind = 'hist')\n",
    "\n",
    "# We add plt.show() to properly render the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layering plots will involve simply calling multiple `.plot()` commands in the same Jupyter cell. We can add some basic styling as well, such as labels, a legend, and a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot commands\n",
    "female['chol'].plot(kind = 'kde')\n",
    "male['chol'].plot(kind = 'kde')\n",
    "\n",
    "\n",
    "# Styling\n",
    "plt.legend([\"Female\", \"Male\"])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Female and Male Cholesterol Levels\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
