{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52dcf6e0-34d7-487a-afc7-0404106c4741",
   "metadata": {},
   "source": [
    "# Day2: EDA & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab26de-efbf-4afd-a962-5346925fbc99",
   "metadata": {},
   "source": [
    "# EDA: Experimental Data Analysis\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the process of analyzing and summarizing the main characteristics of a dataset using statistical and visualization techniques. It helps you understand the data's structure, detect patterns, spot anomalies, and determine relationships among variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c39e82-5d53-45ee-8e51-911571d79711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "df = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bd4dd-b75b-4223-a25e-ac04960eb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee46ae4-2d98-468f-852b-c2b3dcd9f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information about data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c79b6-b922-4b6d-aaa4-e7bc22470be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics of numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017cc91-653e-4162-881b-d3655fa68925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f0b2e-a16c-4390-a621-d53b7deec9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates if necessary\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94de66b-c692-4663-b69b-5bb86a5e3089",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff34b3f-98fa-4ce8-a996-b6bc33f5c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['Age'].hist(bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Chest Pain Type Frequency\n",
    "plt.figure(figsize=(8, 6))\n",
    "df['ChestPainType'].value_counts().plot(kind='bar', color='salmon', edgecolor='black')\n",
    "plt.title('Chest Pain Type Frequency')\n",
    "plt.xlabel('Chest Pain Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc8332-17cc-4867-b8b1-49c7bf95efd5",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ff3c9-f990-430a-b392-554e1348767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Age vs Cholesterol\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Age'], df['Cholesterol'], alpha=0.6, edgecolor='k')\n",
    "plt.title('Age vs Cholesterol')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Cholesterol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e3c94-62dc-4523-b833-a457975abdf2",
   "metadata": {},
   "source": [
    "## Grouped Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4756e67-ab32-466a-a6b6-aa253cc7fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resting Blood Pressure by Chest Pain Type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='ChestPainType', y='RestingBP', data=df, palette='Set2')\n",
    "plt.title('Resting Blood Pressure by Chest Pain Type')\n",
    "plt.xlabel('Chest Pain Type')\n",
    "plt.ylabel('Resting Blood Pressure (mm Hg)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a7f54-0bd6-4ba2-a4a3-f0ddc0577552",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8432b58-e96d-4375-a2a2-438876ec907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot for Cholesterol\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y='Cholesterol', data=df, color='lightblue')\n",
    "plt.title('Cholesterol Distribution and Outliers')\n",
    "plt.ylabel('Cholesterol (mg/dl)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b3020-fed6-4a5a-be99-7db6f5e1feb8",
   "metadata": {},
   "source": [
    "## Patterns and Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb77bb6-bd39-4a65-8a7d-916ecb610a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise-Induced Angina by Heart Disease Status\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='ExerciseAngina', hue='HeartDisease', data=df, palette='pastel')\n",
    "plt.title('Exercise-Induced Angina by Heart Disease Status')\n",
    "plt.xlabel('Exercise Angina')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Heart Disease', loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b58f8d-9b72-4261-9433-a4b34dc232e5",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Preprocessing is an essential step of the machine learning workflow and important for the performance of models. This notebook will introduce the major steps of preprocessing for machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40db6f2-b4dc-4db3-af5c-3da82b98f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83873440-1c4b-45d2-8eb0-61b24cb03398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/heart.csv')\n",
    "# Check out the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ecf9c-5c9f-432a-9f48-0f4074c0a847",
   "metadata": {},
   "source": [
    "Below is a \"data dictionary\", containing information about each of the variables in the dataset.\n",
    "\n",
    "| Feature           | Data Type                    | Description                                                                                                         |\n",
    "|-------------------|-----------------------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "| **Age**           | continuous                 | Age of the patient in years                                                                                        |\n",
    "| **Sex**           | binary discrete (M/F)      | Sex of the patient: M = Male, F = Female                                                                           |\n",
    "| **ChestPainType** | multi-valued discrete (TA, ATA, NAP, ASY) | Type of chest pain: TA = Typical Angina, ATA = Atypical Angina, NAP = Non-Anginal Pain, ASY = Asymptomatic         |\n",
    "| **RestingBP**     | continuous                 | Resting blood pressure measured in mm Hg                                                                           |\n",
    "| **Cholesterol**   | continuous                 | Serum cholesterol level measured in mg/dl                                                                          |\n",
    "| **FastingBS**     | binary discrete (0/1)      | Fasting blood sugar: 1 = Fasting blood sugar > 120 mg/dl, 0 = Fasting blood sugar â‰¤ 120 mg/dl                      |\n",
    "| **RestingECG**    | multi-valued discrete (Normal, ST, LVH) | Resting electrocardiogram results: Normal = Normal, ST = ST-T wave abnormality, LVH = Left ventricular hypertrophy |\n",
    "| **MaxHR**         | continuous                 | Maximum heart rate achieved, numeric value between 60 and 202                                                      |\n",
    "| **ExerciseAngina**| binary discrete (Y/N)      | Presence of exercise-induced angina: Y = Yes, N = No                                                               |\n",
    "| **Oldpeak**       | continuous                 | Depression of the ST segment measured in numeric value (Oldpeak)                                                   |\n",
    "| **ST_Slope**      | multi-valued discrete (Up, Flat, Down) | Slope of the peak exercise ST segment: Up = upsloping, Flat = flat, Down = downsloping                            |\n",
    "| **HeartDisease**  | binary discrete (0/1)      | Output class indicating heart disease: 1 = Presence of heart disease, 0 = Normal                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b2ef7d-df9d-48aa-aef8-d4c1f560c6dc",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's start by getting familiar with our data. This is an important first step before jumping into any modeling.\n",
    "\n",
    "How many samples in the dataset do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d5f84-2c6a-41f3-99ef-a33e89e43a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d457312-7da7-4115-b814-aeffb0716b7c",
   "metadata": {},
   "source": [
    "This is a pretty small dataset.\n",
    "\n",
    "Let's look at the distribution of the target variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9719a77-87b0-4da7-aaf2-10663563754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data['Age'].hist(grid=False, bins=np.linspace(1, 100, 20))\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97e1f7-e6d4-4ca0-827f-643c5ad58a9c",
   "metadata": {},
   "source": [
    "How about how the age correlates with the predictors? We can use the `corr()` function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36beeb6d-8a5d-4076-91b5-d6abeb51cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ac605-bc1b-48a3-be2d-fdbd46d4c939",
   "metadata": {},
   "source": [
    "---\n",
    "### Challenge 1: More EDA\n",
    "\n",
    "Create the following plots, or examine the following distributions, while exploring your data:\n",
    "\n",
    "1. What are column names of this data frame?\n",
    "2. A histogram of the continuous variables.\n",
    "3. What are the unique values of `ExerciseAngina`, and their counts?\n",
    "6. What are the unique `ChestPainType` values, and their counts?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca99a887-a035-46b2-a73f-2fb03ff22b12",
   "metadata": {},
   "source": [
    "# What would be a good machine learning question for this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127490bc-3b1d-413e-9092-3e535795f050",
   "metadata": {},
   "source": [
    "# Creating Train and Test Splits\n",
    "\n",
    "Next, we'll want to split our dataset into training and test data. When creating the model, we need to make sure it only sees the training data. Then, we can examine how well it **generalizes** to data it hasn't seen before. The train and test split is a foundational concept in machine learning. Be sure you're confident you understand why we do this before moving forward!\n",
    "\n",
    "A dataset is often broken up into a feature set, or **design matrix** (typically with the variable name `X`) as well as the target or response variable `y`. Both have $D$ samples, but the design matrix will have a second dimension indicating the number of features we're using for prediction.\n",
    "\n",
    "In this case, we'll extract the output variable `RestingBP` from the data frame to make the `X` and `y` variables. We use a capital `X` to denote it is a `matrix` or 2-D array, and use a lowercase `y` to denote that it is a `vector`, or 1-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae97267-b2b2-47f3-a684-66021a5955cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the response variable and car name\n",
    "X = data.drop(columns=['RestingBP'])\n",
    "# Assign response variable to its own variable\n",
    "y = data['RestingBP'].astype(np.float64)\n",
    "# Confidence check\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51408e63-ea36-42d5-87fe-8710f2c891ef",
   "metadata": {},
   "source": [
    "Now, we perform the train/test split. The package `scikit-learn` is the most commonly used package for machine learning in Python. It provides a function we can easily use to perform this split. Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacf2ad-70d8-4941-8cba-8e56c68c2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6456395-ad1a-4ee0-ac3a-be46139e0146",
   "metadata": {},
   "source": [
    "We commonly do an 80/20 split, where 80% of the data is used for training, and the remaining 20% is used for testing. We can customize this using the parameters of the `train_test_split` function, which you can find in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "We typically split the data randomly. However, sometimes we want this random split to occur in a *reproducible* fashion. This might be when we're testing our code, and want the same random split every time. Or, during a workshop, when we want all participants to get the same split, so that the results look the same for everyone. A reproducible random fit can be done by setting the `random_state`, which is an input argument to `train_test_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899e92f-43c3-4846-a428-60fa593de571",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2ab4b-c68c-4c6b-b0e2-1b4776c1c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X train shape: {X_train.shape}; y train shape: {y_train.shape}')\n",
    "print(f'X test shape: {X_test.shape}; y test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e65b6b-fa2a-4233-8452-1521b65ef625",
   "metadata": {},
   "source": [
    "BEFORE we split the data, there are certain preprocessing tasks we need to do. ORDER MATTERS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75343925-7865-43e6-bba1-f7fff9a673c1",
   "metadata": {},
   "source": [
    "## Missing Data Preprocessing\n",
    "\n",
    "First, let's check to see if there are any missing values in the data set. Missing values are represented by `NaN`. \n",
    "\n",
    "**Question:** In this case, what do missing values stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb04bc-4a44-493f-85d6-739adb1c7d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd318fc2",
   "metadata": {},
   "source": [
    "There is no `NaN` missing values. Is this great?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d613dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddbf498-e812-479b-9ce2-0e574f062164",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f879ac-1c63-4941-90ed-e4ec8327d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RestingBP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1aba5f-637f-420c-a902-82c418d82657",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cholesterol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed852c0",
   "metadata": {},
   "source": [
    "In this case, the `0` represents a missing value, so let's replace those with `np.nan` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51229e5f-c782-49c0-bf22-d2558dc932fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[['RestingBP', 'Cholesterol']]\n",
    "data['RestingBP'].replace(0, np.nan, inplace=True)\n",
    "data['RestingBP'].unique()\n",
    "\n",
    "data['Cholesterol'].replace(0, np.nan, inplace=True)\n",
    "data['Cholesterol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bbd99-c5ba-474b-a194-0003ae520a04",
   "metadata": {},
   "source": [
    "### Imputation\n",
    "\n",
    "In the case of missing values, we have the option to fill in the missing values with the best guess. This is called **imputation**. Here we'll impute any missing values using the average, or mean, of all the data that does exist, as that's the best guess for a data point if all we have is the data itself. To do that we'll use the `SimpleImputer` to assign the mean to all missing values in the data.\n",
    "\n",
    "There are also other strategies that can be used to impute missing data ([see documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)).\n",
    "\n",
    "Let's see how the `SimpleImputer` works on a subset of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30fe06-eb35-48af-88a2-b4cbd74e1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan,\n",
    "                        strategy='mean', \n",
    "                        copy=True)\n",
    "imputed = imputer.fit_transform(data[['RestingBP','Cholesterol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e085f8",
   "metadata": {},
   "source": [
    "Now let's check that the previously null values have been filled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7157f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imputed[data[data['RestingBP'].isna()].index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de080754",
   "metadata": {},
   "source": [
    "### Dropping Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f21878",
   "metadata": {},
   "source": [
    "Another option option is to use `pd.dropna()` to drop `Null` values from the `DataFrame`. This should almost always be used with the `subset` argument which restricts the function to only dropping values that are null in a certain column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db11f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset='Sex')\n",
    "\n",
    "# Now this line will return an empty dataframe\n",
    "data[data['Sex'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e3fff-ded3-4c7a-9dfe-a3b9ff62a566",
   "metadata": {},
   "source": [
    "## Categorical Data Processing\n",
    "\n",
    "`Heart disease` dataset contains both categorical and continuous features, which will each need to be preprocessed in different ways. First, we want to transform the categorical variables from strings to **indicator variables**. Indicator variables have one column per level, For example, the island variable will change from ATA/NAP/ASY/TA --> ATA (1/0), NAP (1/0), ASY (1/0), and TA (1/0). For each set of indicator variables, there should be a 1 in exactly one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ef106-cdee-469a-a337-e7221681474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ST_Slope'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd0d44-440a-4d9e-859b-d818a4c65392",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9bc33e-2b97-4b31-83d1-985dec1e5950",
   "metadata": {},
   "source": [
    " Let's make a list of the categorical variable names to be transformed into indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113d6a3-474c-4b57-9804-8040c38117a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variable names that are categorical for use later\n",
    "cat_var_names = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS']\n",
    "data_cat = data[cat_var_names]\n",
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2d7ba-036f-49e2-ab9e-dc06086eaed6",
   "metadata": {},
   "source": [
    "### Categorical Variable Encoding (One-hot & Dummy)\n",
    "\n",
    "Many machine learning algorithms require that categorical data be encoded numerically in some fashion. There are two main ways to do so:\n",
    "\n",
    "\n",
    "- **One-hot-encoding**, which creates `k` new variables for a single categorical variable with `k` categories (or levels), where each new variable is coded with a `1` for the observations that contain that category, and a `0` for each observation that doesn't. \n",
    "- **Dummy encoding**, which creates `k-1` new variables for a categorical variable with `k` categories\n",
    "\n",
    "However, when using some machine learning algorithms we can run into the so-called [\"Dummy Variable Trap\"](https://www.algosome.com/articles/dummy-variable-trap-regression.html) when using One-Hot-Encoding on multiple categorical variables within the same set of features. This occurs because each set of one-hot-encoded variables can be added together across columns to create a single column of all `1`s, and so are multi-colinear when multiple one-hot-encoded variables exist within a given model. This can lead to misleading results. \n",
    "\n",
    "To resolve this, we can simply add an intercept term to our model (which is all `1`s) and remove the first one-hot-encoded variable for each categorical variables, resulting in `k-1` so-called \"Dummy Variables\". \n",
    "\n",
    "Luckily the `OneHotEncoder` from `sklearn` can perform both one-hot and dummy encoding simply by setting the `drop` parameter (`drop = 'first'` for Dummy Encoding and `drop = None` for One Hot Encoding). \n",
    "\n",
    "**Question:** How many total columns will there be in the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9384a9e-453f-4b62-8bbf-7866b8ac441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dummy_e = OneHotEncoder(categories='auto', drop='first', sparse=False)\n",
    "dummy_e.fit(data_cat);\n",
    "dummy_e.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4091b24-0e57-47e3-a58a-d88826ab5c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dummy_e.transform(data_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec19bc9-6aee-48d1-b043-04ab71e4208b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Continuous Data Preprocessing\n",
    "\n",
    "For numeric data, we don't need to create indicator variables, instead we need to normalize our variables, which helps improve performance of many machine learning models.\n",
    "\n",
    " Let's make subset out the continuous variables to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06511352-4ba4-4bb5-8da4-82430ac080a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_num = data.drop(columns=cat_var_names + ['HeartDisease'])\n",
    "data_num.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13162f8-71d0-4f34-8edb-2b95516b4fa0",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)) is a transformation that puts data into some known \"normal\" scale. We use normalization to improve the performance of many machine learning algorithms (see [here](https://en.wikipedia.org/wiki/Feature_scaling)). There are many forms of normalization, but perhaps the most useful to machine learning algorithms is called the \"z-score\" also known as the standard score. \n",
    "\n",
    "To z-score normalize the data, we simply subtract the mean of the data, and divide by the standard deviation. This results in data with a mean of `0` and a standard deviation of `1`.\n",
    "\n",
    "We'll use the `StandardScaler` from `sklearn` to do normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f872ea-59e4-46a6-b366-578f6d0716a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "norm_e = StandardScaler()\n",
    "norm_e.fit_transform(data_num,).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c20c9",
   "metadata": {},
   "source": [
    "To check the normalization works, let's look at the mean and standard variation of the resulting columns. \n",
    "\n",
    "**Question:** What should the mean and std variation be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean:',norm_e.fit_transform(data_num,).mean(axis=0))\n",
    "print('std:',norm_e.fit_transform(data_num,).std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c54f4",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge 1: Fitting preprocessing functions\n",
    "\n",
    "The simple imputer, normalization and one-hot-encoding rely on sklearn functions that are fit to a data set. \n",
    "\n",
    "1) What is being fit for each of the three functions?\n",
    "    1) One Hot Encoding\n",
    "    2) Standard Scaler\n",
    "    3) Simple Imputer\n",
    "    \n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "When we are preprocessing data we have a few options: \n",
    "1) Fit on the whole data set\n",
    "2) Fit on the training data\n",
    "3) Fit on the testing data\n",
    "\n",
    "Which of the above methods would you use and why?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7c3bf-c215-4de8-830d-c933ed52c505",
   "metadata": {},
   "source": [
    "## Combine it all together\n",
    "\n",
    "Now let's combine what we've learned to preprocess the entire dataset.\n",
    "\n",
    "First we will reload the data set to start with a clean copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b097530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/heart.csv')\n",
    "data['RestingBP'].replace(0, np.nan, inplace=True)\n",
    "data['Cholesterol'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec2c68-beaf-463a-a9a4-f7c91df96354",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split\n",
    "y = data['HeartDisease']\n",
    "X = data.drop('HeartDisease', axis =1, inplace=False)\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=.20, stratify=y)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbadb45c",
   "metadata": {},
   "source": [
    "We want to train our imputers on the training data using `fit_transform`, then `transform` the test data. This more closely resembles what the workflow would look like if you are bringing in brand new test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2be342-483d-4d5b-b3ba-105b60e2cfeb",
   "metadata": {},
   "source": [
    "First, we will subset out the categorical and numerical features separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05022a-a041-4d01-b189-5ceb5e1e0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the categorical and numerical variable column indices\n",
    "cat_var = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope', 'FastingBS']\n",
    "num_var = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "# Splice the training array\n",
    "X_train_cat = X_train[cat_var]\n",
    "X_train_num = X_train[num_var]\n",
    "\n",
    "# Splice the test array\n",
    "X_test_cat = X_test[cat_var]\n",
    "X_test_num = X_test[num_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b746b78-8d31-40e9-819e-2273278c2f88",
   "metadata": {},
   "source": [
    "Now, let's process the categorical data with **Dummy encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d20a3-73b9-490c-9f81-23e37fc09a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Categorical feature encoding\n",
    "X_train_dummy = dummy_e.fit_transform(X_train_cat)\n",
    "X_test_dummy = dummy_e.transform(X_test_cat)\n",
    "\n",
    "\n",
    "# Check the shape\n",
    "X_train_dummy.shape, X_test_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae07768",
   "metadata": {},
   "source": [
    "Now, let's process the numerical data by imputing any missing values and normalizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c7fc4-fd8e-4deb-832a-8e02d82909d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical feature standardization\n",
    "\n",
    "# Impute the data\n",
    "X_train_imp = imputer.fit_transform(X_train_num)\n",
    "X_test_imp = imputer.transform(X_test_num)\n",
    "\n",
    "# Check for missing values\n",
    "np.isnan(X_train_imp).any(), np.isnan(X_test_imp).any()\n",
    "\n",
    "# normalize\n",
    "X_train_norm = norm_e.fit_transform(X_train_imp)\n",
    "X_test_norm = norm_e.transform(X_test_imp)\n",
    "\n",
    "X_train_norm.shape, X_test_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309dc2b-bdf8-420c-a3f3-fe93c854c3eb",
   "metadata": {},
   "source": [
    "Now that we've processed the numerical and categorical data separately, we can put the two arrays back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97ace9-bd20-49c0-bae9-bd629a8b7a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.hstack((X_train_dummy, X_train_norm))\n",
    "X_test = np.hstack((X_test_dummy, X_test_norm))\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931276d0-e6de-47a8-8312-45cc2c66f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_e.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c655c-06f3-458b-8023-f5dd48bcf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab00968",
   "metadata": {},
   "source": [
    "---\n",
    "## Challenge 2: Order of Preprocessing\n",
    "\n",
    "In the preprocessing we did the following steps: \n",
    "\n",
    "1) Null values\n",
    "2) One-hot-encoding\n",
    "3) Imputation\n",
    "4) Normalization\n",
    "\n",
    "Now, consider that we change the order of the steps in the following ways. What effect might that have on the algorithms?\n",
    "**Hint**: Try copying the code from above and trying it out!\n",
    "\n",
    "- One-Hot-Encoding before Null Values\n",
    "- Normalization before Null values\n",
    "\n",
    "**Bonus:** Are there any other switches in order that might affect preprocessing?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4ecff-fb89-4f71-a7ef-70aa43ccc691",
   "metadata": {},
   "source": [
    "Finally, let's save our results as separate `.csv` files, so we won't have to run the preprocessing again.\n",
    "\n",
    "First we will make them DataFrames, add columns, and save them as .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.columns = ['Sex_M', 'ChestPainType_ATA', 'ChestPainType_NAP',\n",
    "       'ChestPainType_TA', 'RestingECG_Normal', 'RestingECG_ST',\n",
    "       'ExerciseAngina_Y', 'ST_Slope_Flat', 'ST_Slope_Up', 'FastingBS_1', \n",
    "                   'Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak' ]\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "X_test.columns = ['Sex_M', 'ChestPainType_ATA', 'ChestPainType_NAP',\n",
    "       'ChestPainType_TA', 'RestingECG_Normal', 'RestingECG_ST',\n",
    "       'ExerciseAngina_Y', 'ST_Slope_Flat', 'ST_Slope_Up', 'FastingBS_1', \n",
    "                   'Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak' ]\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_train.columns = ['HeartDisease']\n",
    "\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.columns = ['HeartDisease']\n",
    "\n",
    "X_train.to_csv('../data/heart_X_train.csv')\n",
    "X_test.to_csv('../data/heart_X_test.csv')\n",
    "y_train.to_csv('../data/heart_y_train.csv')\n",
    "y_test.to_csv('../data/heart_y_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6de745",
   "metadata": {},
   "source": [
    "Although now we will move on to talk about classification, all of the choices we make in the preprocessing pipeline are extremely important to machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
